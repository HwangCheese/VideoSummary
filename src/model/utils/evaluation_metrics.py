# -*- coding: utf-8 -*-
import numpy as np
from scipy import stats

def evaluate_summary(predicted_summary, user_summary, eval_method):
    """ Compare the predicted summary with the user defined one(s).

    :param ndarray predicted_summary: The generated summary from our model.
    :param ndarray user_summary: The user defined ground truth summaries (or summary).
    """

    # ğŸ”¹ user_summaryê°€ 1ì°¨ì› ë°°ì—´ì´ë©´ (T,) â†’ (1, T)ë¡œ ë³€í™˜
    if user_summary.ndim == 1:
        user_summary = user_summary[None, :]  # (1, T)

    # ğŸ”¹ ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš° max ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë°°ì—´ ì´ˆê¸°í™”
    max_len = max(len(predicted_summary), user_summary.shape[1])
    S = np.zeros(max_len, dtype=int)
    G = np.zeros(max_len, dtype=int)

    # ğŸ”¹ `predicted_summary`ê°€ `max_len` ë³´ë‹¤ ì§§ì„ ê²½ìš° íŒ¨ë”©
    S[:len(predicted_summary)] = predicted_summary
    f_scores = []

    for user in range(user_summary.shape[0]):
        G[:user_summary.shape[1]] = user_summary[user]  # GT summary íŒ¨ë”©
        overlapped = S & G  # ê³µí†µëœ ë¶€ë¶„ ì²´í¬

        # ğŸ”¹ Precision, Recall, F1 Score ê³„ì‚°
        precision = sum(overlapped) / (sum(S) + 1e-8)
        recall = sum(overlapped) / (sum(G) + 1e-8)

        if precision + recall == 0:
            f_scores.append(0)
        else:
            f_scores.append((2 * precision * recall * 100) / (precision + recall))

    # ğŸ”¹ `eval_method`ê°€ 'max'ì´ë©´ ìµœëŒ“ê°’, ì•„ë‹ˆë©´ í‰ê·  ì‚¬ìš©
    f_score_result = max(f_scores) if eval_method == 'max' else sum(f_scores) / len(f_scores)

    # ğŸ”¹ Spearman & Kendall í‰ê°€ ì¶”ê°€
    y_pred2 = predicted_summary
    y_true2 = user_summary.mean(axis=0)

    # ğŸ”¹ y_pred2ì™€ y_true2 ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ê°€ì¥ ê¸´ ê¸¸ì´ë¡œ íŒ¨ë”©
    min_len = min(len(y_pred2), len(y_true2))
    y_pred2 = y_pred2[:min_len]
    y_true2 = y_true2[:min_len]

    pS = stats.spearmanr(y_pred2, y_true2)[0]
    kT = stats.kendalltau(stats.rankdata(-np.array(y_true2)), stats.rankdata(-np.array(y_pred2)))[0]

    return f_score_result, kT, pS
