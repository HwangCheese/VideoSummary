import torch
import numpy as np

# âœ… GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device}")

print("torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

if torch.cuda.is_available():
    print("GPU name:", torch.cuda.get_device_name(0))
    print("GPU count:", torch.cuda.device_count())
else:
    print("âŒ CUDA not available. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (GPUì—ì„œ ì‹¤í–‰)
checkpoint = torch.load("dataset/sl_module1_best_f1.pkl", map_location=device)

# âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸°
if "model_state_dict" in checkpoint:
    model_state = checkpoint["model_state_dict"]
elif "state_dict" in checkpoint:
    model_state = checkpoint["state_dict"]
else:
    model_state = checkpoint

# âœ… ê°€ì¤‘ì¹˜ í‰ê· ê°’ ê³„ì‚°
param_means = [p.mean().item() for p in model_state.values()]
print(f"âœ… [GPU] ëª¨ë¸ ê°€ì¤‘ì¹˜ í‰ê· ê°’: {np.mean(param_means):.4f}")

# í™•ì¸ìš© ì˜ˆì‹œ ì½”ë“œ (í•œë²ˆ í™•ì¸ ê¶Œì¥)
checkpoint = torch.load("dataset/sl_module1_best_f1.pkl", map_location="cpu")
print(type(checkpoint))  # dict ë˜ëŠ” OrderedDict í™•ì¸
print(checkpoint.keys())
